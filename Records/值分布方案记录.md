# 值分布 Critic (Distributional Critic) 方案记录

本文档详细记录了当前代码库中 Distributional Critic 的实现原理、方案细节以及其在 PPO 算法中的具体作用。

## 1. 核心理念

传统的 Critic 输出状态 $s$ 的期望回报 $V(s) = \mathbb{E}[R \mid s]$，是一个标量。
**Distributional Critic** 则试图学习回报的完整概率分布 $Z(s)$，即 $Z(s) \stackrel{D}{=} R \mid s$。

引入分布信息的目的通常包括：
1.  **更丰富的训练信号**：分布包含比均值更多的信息（如多峰、方差等），能辅助 Critic 学习更好的特征表示。
2.  **训练稳定性**：在某些场景下，分布式 RL 比标量 RL 收敛更稳定。
3.  **不确定性量化**：虽然目前主要只用了期望，但分布信息本身蕴含了对未来的不确定性预估。

## 2. 实现方案概览

代码通过 `config.distributional` 开关控制，支持三种具体的分布建模模式 (`config.quantile_mode`)：

| 模式 | 全称 | 原理简述 | 输出形式 | 损失函数 |
| :--- | :--- | :--- | :--- | :--- |
| **iqn** (默认) | Implicit Quantile Networks | 将分位点 $\tau \sim U([0,1])$ 作为输入，动态输出对应的分位值。 | 输入 Hidden + $\tau$，输出 Quantile Value | Quantile Huber Loss |
| **fixed** | Fixed Quantile Regression | 类似于 QR-DQN，输出固定分位点（如 0.1, 0.2 ...）对应的分位值。 | 输入 Hidden，输出 $K$ 个 Quantile Values | Quantile Huber Loss |
| **c51** | Categorical 51 | 将回报值域离散化为固定的原子 (Atoms)，预测概率分布。 | 输入 Hidden，输出 $N$ 个 Atoms 的 Logits | Projected Cross Entropy |

### 2.1 架构设计

在 `verl/verl/workers/critic/dp_critic.py` 和 `utils/model.py` 中，根据模式挂载不同的 Head：

*   **Fixed / IQN**:
    *   **Fixed**: 使用 `QRValueHead`，直接输出 $K$ 个标量（分位点的值）。
    *   **IQN**: 使用 `IQNValueHead`，除了 Hidden States 外，还额外接收采样到的 $\tau$ (Taus) 嵌入，输出对应的分位值。
*   **C51**:
    *   使用 `C51ValueHead`，输出 $N$ 个 Logits（对应 $N$ 个固定支撑点 Atoms）。支撑点由 `c51_v_min` 和 `c51_v_max` 决定。

### 2.2 前向传播 (Forward) 与 期望推断

尽管 Critic 内部维护了分布，但 PPO 的 Advantage 计算目前依赖标量 Baseline。因此，在前向推理阶段，我们需要**将分布坍缩为期望**。

*   **IQN / Fixed**:
    *   预测出 $K$ 个分位点的值 $q_i$。
    *   $V(s) \approx \frac{1}{K} \sum_{i=1}^K q_i$ (直接取均值作为期望估计)。
    *   *注：IQN 在推理时也会随机采样 $\tau$，因此其输出的 Baseline 带有一定的随机性（Monte Carlo 估计）。*
*   **C51**:
    *   预测出 Logits，经 Softmax 得到概率 $p_i$。
    *   $V(s) = \sum_{i=1}^N p_i \cdot z_i$，其中 $z_i$ 是固定的 Atom 值。

这一步发生在 `dp_critic.py` 的 `compute_values` 函数中。

### 2.3 训练目标 (Loss)

Critic 的训练目标是最小化预测分布与真实回报 Target 之间的距离。
这里的 Target 是 PPO 采样得到的 **Execute Return** (Monte Carlo Returns) 或者 GAE 估算的 Returns。这是一个标量，但在计算 Loss 时会被视为一个退化的分布（Dirac delta function）。

1.  **IQN / Fixed (Quantile Regression)**
    *   使用 **Quantile Regression Huber Loss**。
    *   目标是让预测的分位点 $q_\tau$ 逼近 Target Return。
    *   公式核心：$\rho_\tau(u) = u \cdot (\tau - \mathbb{I}(u < 0))$，其中 $u = y - q_\tau$。

2.  **C51 (Categorical Projection)**
    *   使用 **Projected Categorical Cross Entropy Loss**。
    *   **投影 (Projection)**: 将标量 Target Return 投影到固定的 Atoms 支撑集上。由于 Return 可能落在两个 Atom 之间，通过线性插值将均值分配给最近的两个 Atom（软 Label）。
    *   **交叉熵**: 计算预测概率 $\log p$ 与投影后的 Target 分布之间的 KL 散度（即 Cross Entropy）。

## 3. 对 PPO 算法公式的影响

在当前的实现中，Distributional Critic **仅改变了 Critic 的训练方式和 Baseline 的估算准确度**，并未修改 PPO Actor 的更新公式。

### 3.1 Actor 更新
标准 PPO Policy Gradient:
$$ \nabla_\theta J(\theta) \approx \mathbb{E} \left[ \min\left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right] $$
其中 Advantage $A_t$ 的计算依赖于 Critic 提供的 $V(s_t)$：
$$ A_t^{GAE} = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$

**Distributional 的影响**：
*   $V(s_t)$ 不再是网络直接输出的标量，而是分布 $Z(s_t)$ 的期望 $\mathbb{E}[Z(s_t)]$。
*   如果 Critic 的分布建模让 $V(s_t)$ 的估计更准确或泛化性更强，那么 $A_t$ 的估计就会更准确，从而间接提升 Policy 的学习效果。

### 3.2 不涉及 Risk-Sensitive
目前代码**没有**使用分布的高阶信息（如 VaR, CVaR）来修改 Advantage。
例如，如果使用 Risk-Averse PPO，可能会定义 $V_{risk}(s) = \text{CVaR}_\alpha[Z(s)]$，但当前实现仅使用了 $\mathbb{E}[Z(s)]$。

## 4. 总结

| 组件 | 标准 PPO Critic | Distributional Critic (当前实现) |
| :--- | :--- | :--- |
| **输出** | 标量 $V$ | 分布 $Z$ (Quantiles 或 Atoms Prob) |
| **推理值** | 直接输出 | $\mathbb{E}[Z]$ (均值) |
| **Loss** | MSE Loss $(V - R)^2$ | Quantile Huber Loss 或 C51 Cross Entropy |
| **作用** | 提供 Baseline | 提供更强 Baseline，更稳定的 Critic 训练 |

此方案主要利用分布强化学习 (Distributional RL) 的辅助任务特性来增强 Critic 的表征能力，而非改变 PPO 的决策风险偏好。

## 5. Risk-Sensitive PPO 扩展 (新增)

为了引入风险偏好（Risk Sensitivity），我们在 Distributional Critic 的基础上增加了 `risk_apply_to` 和 `risk_level` 参数，允许算法利用分布的高阶统计量（如 CVaR）来调整优化目标或 Baseline。

### 5.1 参数定义

*   **`risk_apply_to`**: 决定风险度量 $\rho(Z)$ 作用的位置。
    *   `"none"` (默认): 不启用风险逻辑，使用 $\mathbb{E}[Z]$。
    *   `"baseline"`: 仅替换 Baseline。$A_t = G_t - \rho(Z_t)$。这是一种 **Risk-Aware Baseline**，主要用于方差控制或更稳健的优势估计，不直接改变优化目标 $J(\pi) = \mathbb{E}[G]$。
    *   `"target"`: 替换优化目标。$R_{risk} = \rho(Z_{last})$，$A_t \approx R_{risk}$。这是 **Risk-Sensitive Policy Optimization**，直接最大化风险调整后的回报（如最大化最差情况下的收益）。

*   **`risk_level`**: 定义风险度量函数 $\rho(Z)$。
    *   `"neutral"`: 等价于均值 $\mathbb{E}[Z]$。
    *   `"averse"` / `"cvar_lower_0.1"`: 下尾 CVaR (关注最坏情况)。
    *   `"seeking"` / `"cvar_upper_0.1"`: 上尾 CVaR (关注最好情况)。
    *   支持格式 `cvar_lower_α` 或 `cvar_upper_α`。

### 5.2 实现细节

1.  **风险泛函 (`verl/verl/utils/risk_functional.py`)**:
    *   实现了 `compute_rho_from_dist`，统一处理 Quantile 分布和 C51 分布的 CVaR 计算。
    *   **Quantile**: 筛选 $\tau \le \alpha$ 的分位点取均值。
    *   **C51**: 根据累积概率 CDF 截断 Atoms 并重新加权计算期望。

2.  **Critic 计算 (`dp_critic.py`)**:
    *   在 `compute_values` 中，如果检测到 `risk_level != neutral`，则调用上述函数计算 $\rho(Z)$。
    *   此时 Critic 输出的 `values` Tensor 不再是期望值，而是风险值（如 CVaR）。

3.  **Advantage 计算 (`ray_trainer.py`)**:
    *   **Mode "baseline"**: 利用标准的 GAE 逻辑，但输入的 Baseline $V(s)$ 已经是风险值。
    *   **Mode "target"**: 截取 Critic 对序列末尾的风险预估 $\rho(Z_{last})$，将其广播作为整条序列的回报 Target。

### 5.3 配置示例

在 `ppo_trainer.yaml` 的 `algorithm` 字段中配置：

```yaml
algorithm:
  # 场景 A: 风险规避优化 (优化最差 10% 的情况)
  risk_apply_to: "target"
  risk_level: "cvar_lower_0.1"

  # 场景 B: 仅使用风险基线降低方差
  # risk_apply_to: "baseline"
  # risk_level: "averse"
```

