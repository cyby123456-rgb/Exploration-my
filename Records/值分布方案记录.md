# 值分布 Critic (Distributional Critic) 方案记录

本文档详细记录了当前代码库中 Distributional Critic 的实现原理、方案细节以及其在 PPO 算法中的具体作用。

## 1. 核心理念

传统的 Critic 输出状态 $s$ 的期望回报 $V(s) = \mathbb{E}[R \mid s]$，是一个标量。
**Distributional Critic** 则试图学习回报的完整概率分布 $Z(s)$，即 $Z(s) \stackrel{D}{=} R \mid s$。

引入分布信息的目的通常包括：
1.  **更丰富的训练信号**：分布包含比均值更多的信息（如多峰、方差等），能辅助 Critic 学习更好的特征表示。
2.  **训练稳定性**：在某些场景下，分布式 RL 比标量 RL 收敛更稳定。
3.  **不确定性量化**：虽然目前主要只用了期望，但分布信息本身蕴含了对未来的不确定性预估。

## 2. 实现方案概览

代码通过 `config.distributional` 开关控制，支持三种具体的分布建模模式 (`config.quantile_mode`)：

| 模式 | 全称 | 原理简述 | 输出形式 | 损失函数 |
| :--- | :--- | :--- | :--- | :--- |
| **iqn** (默认) | Implicit Quantile Networks | 将分位点 $\tau \sim U([0,1])$ 作为输入，动态输出对应的分位值。 | 输入 Hidden + $\tau$，输出 Quantile Value | Quantile Huber Loss |
| **fixed** | Fixed Quantile Regression | 类似于 QR-DQN，输出固定分位点（如 0.1, 0.2 ...）对应的分位值。 | 输入 Hidden，输出 $K$ 个 Quantile Values | Quantile Huber Loss |
| **c51** | Categorical 51 | 将回报值域离散化为固定的原子 (Atoms)，预测概率分布。 | 输入 Hidden，输出 $N$ 个 Atoms 的 Logits | Projected Cross Entropy |

### 2.1 架构设计

在 `verl/verl/workers/critic/dp_critic.py` 和 `utils/model.py` 中，根据模式挂载不同的 Head：

*   **Fixed / IQN**:
    *   **Fixed**: 使用 `QRValueHead`，直接输出 $K$ 个标量（分位点的值）。
    *   **IQN**: 使用 `IQNValueHead`，除了 Hidden States 外，还额外接收采样到的 $\tau$ (Taus) 嵌入，输出对应的分位值。
*   **C51**:
    *   使用 `C51ValueHead`，输出 $N$ 个 Logits（对应 $N$ 个固定支撑点 Atoms）。支撑点由 `c51_v_min` 和 `c51_v_max` 决定。

### 2.2 前向传播 (Forward) 与 期望推断

尽管 Critic 内部维护了分布，但 PPO 的 Advantage 计算目前依赖标量 Baseline。因此，在前向推理阶段，我们需要**将分布坍缩为期望**。

*   **IQN / Fixed**:
    *   预测出 $K$ 个分位点的值 $q_i$。
    *   $V(s) \approx \frac{1}{K} \sum_{i=1}^K q_i$ (直接取均值作为期望估计)。
    *   *注：IQN 在推理时也会随机采样 $\tau$，因此其输出的 Baseline 带有一定的随机性（Monte Carlo 估计）。*
*   **C51**:
    *   预测出 Logits，经 Softmax 得到概率 $p_i$。
    *   $V(s) = \sum_{i=1}^N p_i \cdot z_i$，其中 $z_i$ 是固定的 Atom 值。

这一步发生在 `dp_critic.py` 的 `compute_values` 函数中。

### 2.3 训练目标 (Loss)

Critic 的训练目标是最小化预测分布与真实回报 Target 之间的距离。
这里的 Target 是 PPO 采样得到的 **Execute Return** (Monte Carlo Returns) 或者 GAE 估算的 Returns。这是一个标量，但在计算 Loss 时会被视为一个退化的分布（Dirac delta function）。

1.  **IQN / Fixed (Quantile Regression)**
    *   使用 **Quantile Regression Huber Loss**。
    *   目标是让预测的分位点 $q_\tau$ 逼近 Target Return。
    *   公式核心：$\rho_\tau(u) = u \cdot (\tau - \mathbb{I}(u < 0))$，其中 $u = y - q_\tau$。

2.  **C51 (Categorical Projection)**
    *   使用 **Projected Categorical Cross Entropy Loss**。
    *   **投影 (Projection)**: 将标量 Target Return 投影到固定的 Atoms 支撑集上。由于 Return 可能落在两个 Atom 之间，通过线性插值将均值分配给最近的两个 Atom（软 Label）。
    *   **交叉熵**: 计算预测概率 $\log p$ 与投影后的 Target 分布之间的 KL 散度（即 Cross Entropy）。

## 3. 对 PPO 算法公式的影响

在当前的实现中，Distributional Critic **仅改变了 Critic 的训练方式和 Baseline 的估算准确度**，并未修改 PPO Actor 的更新公式。

### 3.1 Actor 更新
标准 PPO Policy Gradient:
$$ \nabla_\theta J(\theta) \approx \mathbb{E} \left[ \min\left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right] $$
其中 Advantage $A_t$ 的计算依赖于 Critic 提供的 $V(s_t)$：
$$ A_t^{GAE} = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$

**Distributional 的影响**：
*   $V(s_t)$ 不再是网络直接输出的标量，而是分布 $Z(s_t)$ 的期望 $\mathbb{E}[Z(s_t)]$。
*   如果 Critic 的分布建模让 $V(s_t)$ 的估计更准确或泛化性更强，那么 $A_t$ 的估计就会更准确，从而间接提升 Policy 的学习效果。

### 3.2 不涉及 Risk-Sensitive
目前代码**没有**使用分布的高阶信息（如 VaR, CVaR）来修改 Advantage。
例如，如果使用 Risk-Averse PPO，可能会定义 $V_{risk}(s) = \text{CVaR}_\alpha[Z(s)]$，但当前实现仅使用了 $\mathbb{E}[Z(s)]$。

## 4. 总结

| 组件 | 标准 PPO Critic | Distributional Critic (当前实现) |
| :--- | :--- | :--- |
| **输出** | 标量 $V$ | 分布 $Z$ (Quantiles 或 Atoms Prob) |
| **推理值** | 直接输出 | $\mathbb{E}[Z]$ (均值) |
| **Loss** | MSE Loss $(V - R)^2$ | Quantile Huber Loss 或 C51 Cross Entropy |
| **作用** | 提供 Baseline | 提供更强 Baseline，更稳定的 Critic 训练 |

此方案主要利用分布强化学习 (Distributional RL) 的辅助任务特性来增强 Critic 的表征能力，而非改变 PPO 的决策风险偏好。
